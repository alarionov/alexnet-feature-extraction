{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with TensorFlow\n",
    "\n",
    "*Transfer learning* is the practice of starting with a network that has already been trained, and then applying that network to your own problem.\n",
    "\n",
    "Because neural networks can often take days or even weeks to train, transfer learning (i.e. starting with a network that somebody else has already spent a lot of time training) can greatly shorten training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "In order to complete this lab, install Python 3, tensorflow, numpy, scipy, matplotlib, and pillow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet\n",
    "Here, you're going to practice transfer learning with [AlexNet](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwiG34CS7vHPAhVKl1QKHW2JAJkQFggcMAA&url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&usg=AFQjCNFlGsSmTUkJw0gLJ0Ry4cm961B7WA&bvm=bv.136593572,d.cGw).\n",
    "\n",
    "AlexNet is a popular base network for transfer learning because its structure is relatively straightforward, it's not too big, and it performs well empirically.\n",
    "\n",
    "Here is a TensorFlow implementation of AlexNet (adapted from [Michael Guerhoy and Davi Frossard](http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import os\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import time\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.ndimage import filters\n",
    "import urllib\n",
    "from numpy import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "train_x = zeros((1, 227,227,3)).astype(float32)\n",
    "train_y = zeros((1, 1000))\n",
    "xdim = train_x.shape[1:]\n",
    "ydim = train_y.shape[1]\n",
    "\n",
    "net_data = load(\"./data/bvlc-alexnet.npy\", encoding=\"latin1\").item()\n",
    "\n",
    "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w,  padding=\"VALID\", group=1):\n",
    "    '''From https://github.com/ethereon/caffe-tensorflow\n",
    "    '''\n",
    "    c_i = input.get_shape()[-1]\n",
    "    assert c_i%group==0\n",
    "    assert c_o%group==0\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "    \n",
    "    \n",
    "    if group==1:\n",
    "        conv = convolve(input, kernel)\n",
    "    else:\n",
    "        input_groups = tf.split(3, group, input)\n",
    "        kernel_groups = tf.split(3, group, kernel)\n",
    "        output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n",
    "        conv = tf.concat(3, output_groups)\n",
    "    return  tf.reshape(tf.nn.bias_add(conv, biases), [-1]+conv.get_shape().as_list()[1:])\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None,) + xdim)\n",
    "resized = tf.image.resize_images(x, (227, 227))\n",
    "\n",
    "def features():\n",
    "\n",
    "    #conv1\n",
    "    #conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "    k_h = 11; k_w = 11; c_o = 96; s_h = 4; s_w = 4\n",
    "    conv1W = tf.Variable(net_data[\"conv1\"][0])\n",
    "    conv1b = tf.Variable(net_data[\"conv1\"][1])\n",
    "    conv1_in = conv(resized, conv1W, conv1b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=1)\n",
    "    conv1 = tf.nn.relu(conv1_in)\n",
    "\n",
    "    #lrn1\n",
    "    #lrn(2, 2e-05, 0.75, name='norm1')\n",
    "    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "    lrn1 = tf.nn.local_response_normalization(conv1,\n",
    "                                                      depth_radius=radius,\n",
    "                                                      alpha=alpha,\n",
    "                                                      beta=beta,\n",
    "                                                      bias=bias)\n",
    "\n",
    "    #maxpool1\n",
    "    #max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "    maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "\n",
    "    #conv2\n",
    "    #conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "    k_h = 5; k_w = 5; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "    conv2W = tf.Variable(net_data[\"conv2\"][0])\n",
    "    conv2b = tf.Variable(net_data[\"conv2\"][1])\n",
    "    conv2_in = conv(maxpool1, conv2W, conv2b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv2 = tf.nn.relu(conv2_in)\n",
    "\n",
    "\n",
    "    #lrn2\n",
    "    #lrn(2, 2e-05, 0.75, name='norm2')\n",
    "    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "    lrn2 = tf.nn.local_response_normalization(conv2,\n",
    "                                                      depth_radius=radius,\n",
    "                                                      alpha=alpha,\n",
    "                                                      beta=beta,\n",
    "                                                      bias=bias)\n",
    "\n",
    "    #maxpool2\n",
    "    #max_pool(3, 3, 2, 2, padding='VALID', name='pool2')                                                  \n",
    "    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "    maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "    #conv3\n",
    "    #conv(3, 3, 384, 1, 1, name='conv3')\n",
    "    k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 1\n",
    "    conv3W = tf.Variable(net_data[\"conv3\"][0])\n",
    "    conv3b = tf.Variable(net_data[\"conv3\"][1])\n",
    "    conv3_in = conv(maxpool2, conv3W, conv3b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv3 = tf.nn.relu(conv3_in)\n",
    "\n",
    "    #conv4\n",
    "    #conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "    k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 2\n",
    "    conv4W = tf.Variable(net_data[\"conv4\"][0])\n",
    "    conv4b = tf.Variable(net_data[\"conv4\"][1])\n",
    "    conv4_in = conv(conv3, conv4W, conv4b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv4 = tf.nn.relu(conv4_in)\n",
    "\n",
    "\n",
    "    #conv5\n",
    "    #conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "    k_h = 3; k_w = 3; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "    conv5W = tf.Variable(net_data[\"conv5\"][0])\n",
    "    conv5b = tf.Variable(net_data[\"conv5\"][1])\n",
    "    conv5_in = conv(conv4, conv5W, conv5b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv5 = tf.nn.relu(conv5_in)\n",
    "\n",
    "    #maxpool5\n",
    "    #max_pool(3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "    maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "    #fc6\n",
    "    #fc(4096, name='fc6')\n",
    "    fc6W = tf.Variable(net_data[\"fc6\"][0])\n",
    "    fc6b = tf.Variable(net_data[\"fc6\"][1])\n",
    "    fc6 = tf.nn.relu_layer(tf.reshape(maxpool5, [-1, int(prod(maxpool5.get_shape()[1:]))]), fc6W, fc6b)\n",
    "\n",
    "    #fc7\n",
    "    #fc(4096, name='fc7')\n",
    "    fc7W = tf.Variable(net_data[\"fc7\"][0])\n",
    "    fc7b = tf.Variable(net_data[\"fc7\"][1])\n",
    "    fc7 = tf.nn.relu_layer(fc6, fc7W, fc7b)\n",
    "    return fc7\n",
    "\n",
    "def logits():\n",
    "    #fc8\n",
    "    #fc(1000, relu=False, name='fc8')\n",
    "    fc8W = tf.Variable(net_data[\"fc8\"][0])\n",
    "    fc8b = tf.Variable(net_data[\"fc8\"][1])\n",
    "    fc8 = tf.nn.xw_plus_b(features(), fc8W, fc8b)\n",
    "    return fc8\n",
    "\n",
    "def probabilities():\n",
    "    #prob\n",
    "    #softmax(name='prob'))\n",
    "    return tf.nn.softmax(logits())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet Inference\n",
    "\n",
    "![alt text](poodle.png \"Poodle\")\n",
    "![alt text](weasel.png \"Weasel\")\n",
    "\n",
    "To start, run a few ImageNet images through the network, and verify that the network classifies them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0\n",
      "miniature poodle: 0.389\n",
      "toy poodle: 0.223\n",
      "Bedlington terrier: 0.173\n",
      "standard poodle: 0.150\n",
      "komondor: 0.026\n",
      "\n",
      "Image 1\n",
      "weasel: 0.331\n",
      "polecat, fitch, foulmart, foumart, Mustela putorius: 0.280\n",
      "black-footed ferret, ferret, Mustela nigripes: 0.210\n",
      "mink: 0.081\n",
      "Arctic fox, white fox, Alopex lagopus: 0.027\n",
      "\n",
      "Time: 0.095 seconds\n"
     ]
    }
   ],
   "source": [
    "# NOTE: You don't need to edit this code.\n",
    "\n",
    "from caffe_classes import class_names\n",
    "\n",
    "# Initialize the Model\n",
    "prob = probabilities()\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = (imread(\"poodle.png\")[:,:,:3]).astype(float32)\n",
    "im1 = im1 - mean(im1)\n",
    "\n",
    "im2 = (imread(\"weasel.png\")[:,:,:3]).astype(float32)\n",
    "im2 = im2 - mean(im2)\n",
    "\n",
    "# Run Inference\n",
    "t = time.time()\n",
    "output = sess.run(prob, feed_dict = {x:[im1,im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = argsort(output)[input_im_ind,:]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (class_names[inds[-1-i]], output[input_im_ind, inds[-1-i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Sign Inference\n",
    "![alt text](construction.jpg \"Construction Sign\")\n",
    "![alt text](stop.jpg \"Stop Sign\")\n",
    "\n",
    "Next, run two of the traffic sign images through the network, and see how well the classifier performs.\n",
    "\n",
    "You'll notice, however, that the AlexNet model expects a 227x227x3 pixel image, whereas the traffic sign images are 32x32x3 pixels.\n",
    "\n",
    "In order to feed our the traffic sign images into AlexNet, you'll need to resize the images to the dimensions that AlexNet expects.\n",
    "\n",
    "You could resize the images outside of this program, but that would make for a huge collection of images. Instead, use the `tf.images.resize_images()` method to resize the images within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0\n",
      "screen, CRT screen: 0.051\n",
      "digital clock: 0.041\n",
      "laptop, laptop computer: 0.030\n",
      "balance beam, beam: 0.027\n",
      "parallel bars, bars: 0.023\n",
      "\n",
      "Image 1\n",
      "digital watch: 0.395\n",
      "digital clock: 0.275\n",
      "bottlecap: 0.115\n",
      "stopwatch, stop watch: 0.104\n",
      "combination lock: 0.086\n",
      "\n",
      "Time: 0.096 seconds\n"
     ]
    }
   ],
   "source": [
    "from caffe_classes import class_names\n",
    "\n",
    "# TODO: Update the xdim, x, and resized variables to accomodate 32x32x3 pixel images.\n",
    "xdim = (32,32,3)\n",
    "x = tf.placeholder(tf.float32, (None,) + xdim)\n",
    "resized = tf.image.resize_images(x, (227, 227))\n",
    "\n",
    "# NOTE: You don't need to edit the code below.\n",
    "# Initialize the Model\n",
    "prob = probabilities()\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = (imread(\"construction.jpg\")[:,:,:3]).astype(float32)\n",
    "im1 = im1 - mean(im1)\n",
    "\n",
    "im2 = (imread(\"stop.jpg\")[:,:,:3]).astype(float32)\n",
    "im2 = im2 - mean(im2)\n",
    "\n",
    "# Run Inference\n",
    "t = time.time()\n",
    "\n",
    "output = sess.run(prob, feed_dict = {x: [im1, im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = argsort(output)[input_im_ind,:]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (class_names[inds[-1-i]], output[input_im_ind, inds[-1-i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "The problem is that AlexNet was trained on the [ImageNet](http://www.image-net.org/) database, which has 1000 classes of images. You can see the classes in the `caffe_classes.py` file. None of those classes involves traffic signs.\n",
    "\n",
    "In order to successfully classify our traffic sign images, you need to remove the final, 1000-neuron classification layer and replace it with a new, 43-neuron classification layer.\n",
    "\n",
    "This is called feature extraction, because you're basically extracting the images features captured by the penultimate layer, and passing them to a new classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0\n",
      "41: 0.610\n",
      "27: 0.147\n",
      "30: 0.077\n",
      "1: 0.041\n",
      "33: 0.035\n",
      "\n",
      "Image 1\n",
      "38: 0.401\n",
      "30: 0.196\n",
      "10: 0.057\n",
      "27: 0.052\n",
      "12: 0.050\n",
      "\n",
      "Time: 0.092 seconds\n"
     ]
    }
   ],
   "source": [
    "# TODO: Redefine the logits() function to create a new fully-connected layer.\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
    "\n",
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "def logits():\n",
    "    #fc8\n",
    "    #fc(1000, relu=False, name='fc8')\n",
    "    fc8W = tf.Variable(tf.truncated_normal(shape=[4096,43], stddev=0.05))\n",
    "    fc8b = tf.Variable(tf.constant(0.05, shape=[43]))\n",
    "    fc8 = tf.nn.xw_plus_b(features(), fc8W, fc8b)\n",
    "    return fc8\n",
    "\n",
    "# NOTE: You don't need to edit the code below.\n",
    "# Initialize the Model\n",
    "prob = probabilities()\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = (imread(\"construction.jpg\")[:,:,:3]).astype(float32)\n",
    "im1 = im1 - mean(im1)\n",
    "\n",
    "im2 = (imread(\"stop.jpg\")[:,:,:3]).astype(float32)\n",
    "im2 = im2 - mean(im2)\n",
    "\n",
    "# Run Inference\n",
    "t = time.time()\n",
    "output = sess.run(prob, feed_dict = {x:[im1,im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = argsort(output)[input_im_ind,:]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (inds[-1-i], output[input_im_ind, inds[-1-i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Feature Extractor\n",
    "The feature extractor you just created works, in the sense that data will flow through the network and result in predictions.\n",
    "\n",
    "But the predictions aren't accurate, because you haven't yet trained the new classification layer.\n",
    "\n",
    "In order to do that, you'll need to read in the training dataset and train the network with cross entropy.\n",
    "\n",
    "Notice that in the network definition (look in the `features()` function), all of the layers are set to `trainable=False`. This freezes the weights of those layers, so you keep the trained AlexNet features and only train the final classification layer. This also makes training faster.\n",
    "\n",
    "Training AlexNet (even just the final layer!) can take a little while, so it can be helpful to try out your code using only a small portion of the training set. Once you're confident your implementation works, you can train use the entire training dataset to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:      1, Training Accuracy:   3.1%, start:      0, end:     64\n",
      "Optimization Iteration:    101, Training Accuracy:   1.6%, start:   6400, end:   6464\n",
      "Optimization Iteration:    201, Training Accuracy:   1.6%, start:  12800, end:  12864\n",
      "Optimization Iteration:    301, Training Accuracy:   0.0%, start:  19200, end:  19264\n",
      "Optimization Iteration:    401, Training Accuracy:   3.1%, start:  25600, end:  25664\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'timedelta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4fd7b342cfc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# Print the time-usage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time usage: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_dif\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mfeed_dict_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timedelta' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TODO: Load the training dataset.\n",
    "training_file = './data/train.p'\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "X_train, y_train = train['features'].astype(float32), train['labels']\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "X_small = X_train[0:9]\n",
    "y_small = y_train[0:9]\n",
    "\n",
    "# TODO: Pre-process the input data.\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(y_train)\n",
    "labels = encoder.transform(y_train)\n",
    "labels_small = encoder.transform(y_small)\n",
    "\n",
    "X_small = X_small - mean(X_small)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_features, valid_features, train_labels, valid_labels = \\\n",
    "    train_test_split(X_train, labels, test_size=0.33, random_state=1)\n",
    "\n",
    "# TODO: Once you are confident that the training works, update the training set to use all of the data.\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 43], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits(), labels=y_true)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(loss)\n",
    "\n",
    "y_pred = tf.nn.softmax(logits())\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "prob = probabilities()\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "feed_dict_train = {x: X_small, y_true: labels_small} #99 training samples\n",
    "train_accuracy  = sess.run(accuracy, feed_dict = feed_dict_train)\n",
    "\n",
    "# TODO: Train the network.\n",
    "total_iterations = 0\n",
    "train_batch_size = 64\n",
    "# Start-time used for printing time-usage below.\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(ceil(train_features.shape[0]/train_batch_size)):\n",
    "    start = i * train_batch_size \n",
    "    if start > train_features.shape[0]:\n",
    "        break\n",
    " \n",
    "    end = min((i + 1) * train_batch_size, train_features.shape[0])\n",
    "\n",
    "    # Get a batch of training examples.\n",
    "    # x_batch now holds a batch of images and\n",
    "    # y_true_batch are the true labels for those images.\n",
    "    x_batch      = train_features[start:end] #.reshape(-1, 32*32*3)\n",
    "    y_true_batch = train_labels[start:end]\n",
    "\n",
    "    # Put the batch into a dict with the proper names\n",
    "    # for placeholder variables in the TensorFlow graph.\n",
    "    feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "\n",
    "    # Run the optimizer using this batch of training data.\n",
    "    # TensorFlow assigns the variables in feed_dict_train\n",
    "    # to the placeholder variables and then runs the optimizer.\n",
    "    sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "    # Print status every 100 iterations.\n",
    "    if i % 100 == 0:\n",
    "        # Calculate the accuracy on the training-set.\n",
    "        acc = sess.run(accuracy, feed_dict=feed_dict_train)\n",
    "        # Message for printing.\n",
    "        msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}, start: {2:>6}, end: {3:>6}\"\n",
    "        # Print it.\n",
    "        print(msg.format(i + 1, acc, start, end))\n",
    "\n",
    "# Ending time.\n",
    "end_time = time.time()\n",
    "\n",
    "# Difference between start and end-times.\n",
    "time_dif = end_time - start_time\n",
    "\n",
    "# Print the time-usage.\n",
    "#print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "feed_dict_val = {x: valid_features, y_true: valid_labels}\n",
    "val_accuracy  = sess.run(accuracy, feed_dict=feed_dict_val)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:      1, Training Accuracy:   3.1%, start:      0, end:     64\n"
     ]
    }
   ],
   "source": [
    "for e in range(40):\n",
    "    for i in range(ceil(train_features.shape[0]/train_batch_size)):\n",
    "        start = i * train_batch_size \n",
    "        if start > train_features.shape[0]:\n",
    "            break\n",
    "\n",
    "        end = min((i + 1) * train_batch_size, train_features.shape[0])\n",
    "\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "        x_batch      = train_features[start:end] #.reshape(-1, 32*32*3)\n",
    "        y_true_batch = train_labels[start:end]\n",
    "\n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "\n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every 100 iterations.\n",
    "        if i % 100 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = sess.run(accuracy, feed_dict=feed_dict_train)\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}, start: {2:>6}, end: {3:>6}\"\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, acc, start, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict_val = {x: valid_features, y_true: valid_labels}\n",
    "val_accuracy  = sess.run(accuracy, feed_dict=feed_dict_val)\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Accuracy:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "You've trained AlexNet as a feature extractor!\n",
    "\n",
    "Don't be discouraged if your validation accuracy still isn't as high as you'd like.\n",
    "\n",
    "Coming up, you'll explore other networks to use for transfer learning, as well as approaches to improve accuracy."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [keras]",
   "language": "python",
   "name": "Python [keras]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
